{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Deep Learning Workshop − Exercise\n",
    "\n",
    "ODSC East − April 15, 2020\n",
    "\n",
    "Mark Steadman, Viktor Kovryzhkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions \n",
    "Functions for creating sliding windows, splitting data into train/test partitions, metrics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(arr, window_size):\n",
    "    \"\"\" Create sliding windows on array \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr: np.ndarray\n",
    "        array to reshape\n",
    "    window_size: int\n",
    "        sliding window size\n",
    "    Returns\n",
    "    -------\n",
    "    new_arr: np.ndarray\n",
    "        2D matrix of shape (arr.shape[0] - window_size + 1, window_size)\n",
    "    \"\"\"\n",
    "    (stride,) = arr.strides\n",
    "    arr = np.lib.index_tricks.as_strided(\n",
    "        arr,\n",
    "        (arr.shape[0] - window_size + 1, window_size),\n",
    "        strides=[stride, stride],\n",
    "        writeable=False,\n",
    "    )\n",
    "    return arr\n",
    "\n",
    "\n",
    "def prepare_data(y, fdw, fw, test_size=0.1, take=None):\n",
    "    \"\"\" General data preparation function. \n",
    "    Takes array of targets and does following steps: \n",
    "    1) Removes nan valies \n",
    "    2) Creates sliding windows using feature derivation and forecast windows\n",
    "    3) Splits data into train and test partitions\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.ndarray\n",
    "        Target series \n",
    "    fdw: int \n",
    "        Feature derivation window \n",
    "    fw: int \n",
    "        Forecast window \n",
    "    test_size: float \n",
    "        Size of the validation partition\n",
    "    take: int\n",
    "        Number of rows to take from original array. Useful for fast experiments of dataset is large. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_train: np.ndarray\n",
    "        training X matrix of shape (?, FDW, 1)\n",
    "    y_train: np.ndarray\n",
    "        training y matrix of shape (?, FW)\n",
    "    x_test: np.ndarray\n",
    "        testing X matrix of shape (?, FDW, 1)\n",
    "    y_test: np.ndarray\n",
    "        testing y matrix of shape (?, FW, 1)\n",
    "    \"\"\"\n",
    "    y = y[~pd.isnull(y)]\n",
    "    \n",
    "    if take is not None: \n",
    "        y = y[-take:]\n",
    "    \n",
    "    windows = sliding_window(y, fdw + fw)\n",
    "        \n",
    "    num_test_batches = int(windows.shape[0] * test_size)\n",
    "    train_windows = windows[:-num_test_batches]\n",
    "    test_windows = windows[-num_test_batches:]\n",
    "\n",
    "    x_train = train_windows[:, :fdw, np.newaxis]\n",
    "    y_train = train_windows[:, fdw:]\n",
    "\n",
    "    x_test = test_windows[:, :fdw, np.newaxis]\n",
    "    y_test = test_windows[:, fdw:]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "\n",
    "def simple_baseline(x, y):\n",
    "    \"\"\" Simple naive baseline predictions. \n",
    "    For each row in y predicts latest known value. \n",
    "    \"\"\"\n",
    "    latest = x[:, -1, :]\n",
    "    y_pred = np.tile(latest, (1, y.shape[1]))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def seasonal_baseline(x, y, period): \n",
    "    \"\"\" Seasonal naive baseline predictions. \n",
    "    \"\"\"\n",
    "    indexes = np.arange(y.shape[1]) - period\n",
    "    return np.squeeze(x_test[:, indexes])\n",
    "\n",
    "\n",
    "def mase(y_true, y_pred, y_naive_pred):\n",
    "    baseline_mae = mean_absolute_error(y_true, y_naive_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mae / baseline_mae\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred): \n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "def plot_predictions(x, y_true, y_pred): \n",
    "    x_indexes = np.arange(x.shape[0])\n",
    "    y_indexes = np.arange(x.shape[0], x.shape[0] + y_true.shape[0])\n",
    "    line1 = plt.plot(x_indexes, x)\n",
    "    line2 = plt.plot(y_indexes, y_true)\n",
    "    line3 = plt.plot(y_indexes, y_pred)\n",
    "    \n",
    "    line1[0].set_label('Historical data')\n",
    "    line2[0].set_label('Ground truth')\n",
    "    line3[0].set_label('Forecast')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def plot_loss(history): \n",
    "    loss = history['loss']\n",
    "    line = plt.plot(range(len(loss)), loss)\n",
    "    line[0].set_label('Training loss')\n",
    "    if 'val_loss' in history: \n",
    "        val_loss = history['val_loss']\n",
    "        line = plt.plot(range(len(val_loss)), val_loss)\n",
    "        line[0].set_label('Validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "class ScalerWrapper(object):\n",
    "    \"\"\" Helped wrapper class to help scale multi dimentional matrices usings scale.  \n",
    "    \"\"\"\n",
    "    def __init__(self, scaler):\n",
    "        self.scaler = scaler \n",
    "        \n",
    "    def fit(self, X): \n",
    "        self.scaler.fit(np.reshape(X, (-1, 1)))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        original_shape = X.shape\n",
    "        X_scaled = self.scaler.transform(np.reshape(X, (-1, 1)))\n",
    "        return np.reshape(X_scaled, original_shape)\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        original_shape = X.shape\n",
    "        X_unscaled = self.scaler.inverse_transform(np.reshape(X, (-1, 1)))\n",
    "        return np.reshape(X_unscaled, original_shape)\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example dataset\n",
    "\n",
    "NYC hourly energy consumption dataset.  \n",
    "Has strong daily seasonality and weekly seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/nyc_energy.csv')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df.index[:500], df['demand'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup \n",
    "\n",
    "Dataset has weekly seasonality, that's why in order to capture it we need to learn from the whole week.   \n",
    "Thus we will be using feature derivation window equal to 168 time steps (168 hours in a week) and forecast window of 24 hours to predict for the next day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the feature derivation window or number of time steps to look back \n",
    "FDW = 168\n",
    "# size of the forecast window or number of time steps to predict into the future \n",
    "FW = 24\n",
    "# size of the test set \n",
    "test_size = 0.1\n",
    "# number of rows use for training and testing. Set this to None to use entire dataset (~40k rows)\n",
    "nrows = 5000 \n",
    "\n",
    "# name of the target column in csv file\n",
    "target = 'demand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and create sliding windows \n",
    "\n",
    "x_train, y_train, x_test, y_test = prepare_data(df[target].values, fdw=FDW, fw=FW, test_size=test_size, take=nrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how one of the sliding window looks like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "train_line = plt.plot(np.arange(FDW), x_train[0])\n",
    "train_line[0].set_label('Historical data')\n",
    "test_line = plt.plot(np.arange(FDW, FDW + FW), y_train[0])\n",
    "test_line[0].set_label('Future data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's evaluate baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple naive baseline outputs latest known value, i.e. last value in historical sequence.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_simple_pred = simple_baseline(x_test, y_test)\n",
    "print('Simple baseline')\n",
    "print('\\tRMSE {}'.format(rmse(y_test, y_simple_pred)))\n",
    "print('\\tMAE  {}'.format(mean_absolute_error(y_test, y_simple_pred)))\n",
    "\n",
    "plot_predictions(x_test[0], y_test[0], y_simple_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset has daily seasonality, i.e. the pattern repeats every day - enery consumption rises during the day and decreases during the night.  \n",
    "In this case baseline which outputs latest value for the same hour of previous day will be better.  \n",
    "For example energy consumption at 5PM today will likely be more similar to energy consumption 5PM yesterday, rather then an hour ago. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_24h_pred = seasonal_baseline(x_test, y_test, period=24)\n",
    "print('24h seasonal baseline')\n",
    "print('\\tRMSE {}'.format(rmse(y_test, y_24h_pred)))\n",
    "print('\\tMAE  {}'.format(mean_absolute_error(y_test, y_24h_pred)))\n",
    "\n",
    "plot_predictions(x_test[0], y_test[0], y_24h_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset also exposes weekly seasonality with some repeating pattern on weekends.  \n",
    "Thus another baseline worth comparing is baseline which outputs prediction for the same hour of day last day of week.  \n",
    "In another words 1PM on Saturday will likely be more similar to 1PM previos Saturday rather then 1PM Friday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_168h_pred = seasonal_baseline(x_test, y_test, period=168)\n",
    "print('168h (1week) seasonal baseline')\n",
    "print('\\tRMSE {}'.format(rmse(y_test, y_168h_pred)))\n",
    "print('\\tMAE  {}'.format(mean_absolute_error(y_test, y_168h_pred)))\n",
    "\n",
    "plot_predictions(x_test[0], y_test[0], y_168h_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural nets \n",
    "\n",
    "Now it is time to train some models.  \n",
    "\n",
    "There are two very simple LSTM based models avaliable here. \n",
    "1) SimpleLSTMEstimator - network with LSTM encoder and dense layer which outputs predictions for all time steps into the future.  \n",
    "2) SimpleSeq2SeqLSTMEstimator - network which uses LSTM layers for both encoder and decoder in a way similar to sequence-to-sequence architecture. This architecture doesn't use rolling structure of the decoder prediction, just passes last encoder output into decoder at every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTMEstimator(object): \n",
    "    def __init__(self, fdw, fw, lstm_units=None, scaler=None): \n",
    "        self.fdw = fdw\n",
    "        self.fw = fw\n",
    "        self.lstm_units = lstm_units\n",
    "        self.scaler = scaler \n",
    "\n",
    "    def build_model(self): \n",
    "        inputs = tf.keras.Input(shape=[self.fdw, 1], name='inputs')\n",
    "        \n",
    "        rnn_cells = []\n",
    "        for units in self.lstm_units:\n",
    "            cell = tf.keras.layers.LSTMCell(units)\n",
    "            rnn_cells.append(cell)\n",
    "        outputs = tf.keras.layers.RNN(rnn_cells)(inputs)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(self.fw, activation='linear')(outputs)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "    def fit(self, X, y, optimizer='adam', loss='mae', **kwargs): \n",
    "        if self.scaler: \n",
    "            X = self.scaler.fit_transform(X)\n",
    "            y = self.scaler.transform(y)\n",
    "\n",
    "        self.build_model()\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "        \n",
    "        \n",
    "        return self.model.fit(X, y, **kwargs)\n",
    "        \n",
    "    def predict(self, X, **kwargs): \n",
    "        if self.scaler: \n",
    "            X = self.scaler.transform(X)\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        \n",
    "        if self.scaler: \n",
    "            predictions = self.scaler.inverse_transform(predictions)        \n",
    "        return predictions\n",
    "\n",
    "    \n",
    "class SimpleSeq2SeqLSTMEstimator(object): \n",
    "    \n",
    "    def __init__(self, fdw, fw, lstm_units=None, scaler=None): \n",
    "        self.fdw = fdw\n",
    "        self.fw = fw\n",
    "        self.lstm_units = lstm_units\n",
    "        self.scaler = scaler \n",
    "        \n",
    "    def build_model(self): \n",
    "        inputs = tf.keras.Input(shape=[self.fdw, 1], name='inputs')\n",
    "        \n",
    "        encoder_cells = []\n",
    "        for units in self.lstm_units:\n",
    "            cell = tf.keras.layers.LSTMCell(units)\n",
    "            encoder_cells.append(cell)\n",
    "        outputs = tf.keras.layers.RNN(encoder_cells)(inputs)\n",
    "        \n",
    "        outputs = tf.keras.layers.RepeatVector(self.fw)(outputs)\n",
    "\n",
    "        decoder_cells = []\n",
    "        for units in self.lstm_units:\n",
    "            cell = tf.keras.layers.LSTMCell(units)\n",
    "            decoder_cells.append(cell)\n",
    "        outputs = tf.keras.layers.RNN(decoder_cells, return_sequences=True)(outputs)\n",
    "        \n",
    "        outputs = tf.keras.layers.Dense(1, activation='linear')(outputs)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        \n",
    "    def fit(self, X, y, optimizer='adam', loss='mae', **kwargs): \n",
    "        if self.scaler: \n",
    "            X = self.scaler.fit_transform(X)\n",
    "            y = self.scaler.transform(y)\n",
    "            \n",
    "        self.build_model()\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "        return self.model.fit(X, y, **kwargs)\n",
    "        \n",
    "    def predict(self, X, **kwargs): \n",
    "        if self.scaler: \n",
    "            X = self.scaler.transform(X)\n",
    "            \n",
    "        predictions = self.model.predict(X)\n",
    "        \n",
    "        if self.scaler: \n",
    "            predictions = self.scaler.inverse_transform(predictions)                \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup \n",
    "epochs = 10\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "validation_split = 0.1\n",
    "\n",
    "# 1 for verbose training output (loss and val_loss for each epoch)\n",
    "verbose = 0\n",
    "\n",
    "# Number of LSTM units. Add elements to the list to create stacked LSTMs. \n",
    "lstm_units = [64] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SimpleLSTMEstimator(\n",
    "    fdw=FDW,\n",
    "    fw=FW,\n",
    "    lstm_units=lstm_units,\n",
    "    scaler=ScalerWrapper(StandardScaler()),\n",
    ")\n",
    "history = estimator.fit(\n",
    "    x_train, y_train, \n",
    "    optimizer=tf.keras.optimizers.Adam(0.01), \n",
    "    loss='mae', \n",
    "    shuffle=True, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = SimpleSeq2SeqLSTMEstimator(\n",
    "    fdw=FDW,\n",
    "    fw=FW,\n",
    "    lstm_units=lstm_units,\n",
    "    scaler=ScalerWrapper(StandardScaler()),\n",
    ")\n",
    "history = seq2seq.fit(\n",
    "    x_train, y_train, \n",
    "    optimizer=tf.keras.optimizers.Adam(0.01), \n",
    "    loss='mae', \n",
    "    shuffle=True, \n",
    "    epochs=epochs, \n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose,\n",
    "    validation_split=validation_split,\n",
    ")\n",
    "plot_loss(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = estimator.predict(x_test)\n",
    "y_pred = np.squeeze(y_pred)\n",
    "print('Simple LSTM model')\n",
    "print('\\tRMSE        {}'.format(rmse(y_test, y_pred)))\n",
    "print('\\tMAE         {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('\\tMASE latest {}'.format(mase(y_test, y_pred, y_simple_pred)))\n",
    "print('\\tMASE 24h    {}'.format(mase(y_test, y_pred, y_24h_pred)))\n",
    "print('\\tMASE 168h   {}'.format(mase(y_test, y_pred, y_168h_pred)))\n",
    "plot_predictions(x_test[i], y_test[i], y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = seq2seq.predict(x_test)\n",
    "y_pred = np.squeeze(y_pred)\n",
    "print('Simple LSTM model')\n",
    "print('\\tRMSE        {}'.format(rmse(y_test, y_pred)))\n",
    "print('\\tMAE         {}'.format(mean_absolute_error(y_test, y_pred)))\n",
    "print('\\tMASE latest {}'.format(mase(y_test, y_pred, y_simple_pred)))\n",
    "print('\\tMASE 24h    {}'.format(mase(y_test, y_pred, y_24h_pred)))\n",
    "print('\\tMASE 168h   {}'.format(mase(y_test, y_pred, y_168h_pred)))\n",
    "plot_predictions(x_test[i], y_test[i], y_pred[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Dataset &minus; Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/stock_close_prices.csv')\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(df.index, df['GOOG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FDW = 30\n",
    "FW = 7\n",
    "test_size = 0.2\n",
    "target = 'GOOG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test and create sliding windows \n",
    "x_train, y_train, x_test, y_test = prepare_data(df[target].values, fdw=FDW, fw=FW, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use simple latest baseline for this dataset cause it doesn't expose really strong seasonal pattern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_simple_pred = simple_baseline(x_test, y_test)\n",
    "print('Simple baseline')\n",
    "print('\\tRMSE {}'.format(rmse(y_test, y_simple_pred)))\n",
    "print('\\tMAE  {}'.format(mean_absolute_error(y_test, y_simple_pred)))\n",
    "\n",
    "plot_predictions(x_test[0], y_test[0], y_simple_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_workshop",
   "language": "python",
   "name": "ts_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
